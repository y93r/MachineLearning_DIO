{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exemplo 2 - Deep Learning do 0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPxjIUezn11ig+fW3QfX7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y93r/MachineLearning_DIO/blob/main/Exemplo_2_Deep_Learning_do_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando bibliotecas necessárias para criar rede neural"
      ],
      "metadata": {
        "id": "Fp6wJbiG4NG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7ZlAR_2M2vUs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XBxrH_nz4YQP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão de img para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./ MNIST_data/',download=True,train=True,transform=transform) #Carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader (trainset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por parte \n",
        "\n",
        "valset = datasets.MNIST('./ MNIST_data/',download=True,train=False,transform=transform)#Carrega a parte de validação\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados partes"
      ],
      "metadata": {
        "id": "oxmw6EMW61T8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens,etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(),cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vItLB5eU8e1m",
        "outputId": "a350ed46-6890-4223-d69d-2b342e5b3987"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J5SmmFA8ySM",
        "outputId": "74bd7757-5cd8-4240-9466-020288f94bf2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(Modelo,self).__init__()\n",
        "        self.lineari=nn.Linear(28*28,128) #camada de entrada,784 neurônios que se ligam a 128\n",
        "        self.linear2=nn.Linear(128,64) #camada interna1,128 neurônios que se ligam a 64\n",
        "        self.linear3=nn.Linear(64,10) #camada interna2,64 neurônios que se ligam a 10\n",
        "        # para a camada de saida não é necessário definir nada pois só precisamos pegar o output da camada interna2\n",
        "   def forward(self,X):\n",
        "        X=F.relu(self.linear1(X)) #função de ativação da camada de entrada paraacamada interna1\n",
        "        X=F.relu(self.linear2(X)) #função de ativação da camada interna1 para a camada interna2\n",
        "        X=self.linear3(X) #função de ativação da camada interna2 para a camada de saída,nesse caso f(x)=x\n",
        "        return F.log_softmax(X,dim=1) #dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "UKzMSqAl9OLB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador=optim.SGD(modelo.parameters(),lr=0.01,momentum=0.5) #define a política de atualização dos pesos\n",
        "  inicio=time() #timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() #definindo o criterio para calcular a perda\n",
        "  EPOCHS=10 #numero de epochs que o algoritmo rodará\n",
        "  modelo.train() #ativando o modo de treinamento do modelo\n",
        "                                                                                                                                        \n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada= 0 #inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens,etiquetas in trainloader:\n",
        "        imagens=imagens.view(imagens.shape[0],-1) #convertendo as imagens para\"vetores\"de 28*28 casas para ficarem compatíveis com a\n",
        "        otimizador.zero_grad() #zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "        output=modelo(imagens.to(device)) #colocando os dados no modelo\n",
        "        perda_instantanea=criterio(output,etiquetas.to(device)) #calculandoaperda da epoch em questão\n",
        "        \n",
        "        perda_instantanea.backward() #back propagation a partir da perda\n",
        "        otimizador.step() #atualizando os pesos e a bias\n",
        "        perda_acumulada += perda_instantanea.item() #atualização da perda acumulada\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch{} - Perda resultante:{}\".format(epoch+1,perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino(em minutos)= \",(time()-inicio)/60)"
      ],
      "metadata": {
        "id": "z2NTF9IjAICu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo,valloader,device):\n",
        "    conta_corretas,conta_todas = 0, 0\n",
        "    for imagens,etiquetas in valloader:\n",
        "      for i in range(len(etiquetas)):\n",
        "        img = imagens[i].view(1,784)\n",
        "        #desativar o autograd para acelerar a validação.Grafos computacionais dinámicos tem um custo alto de processamento\n",
        "        with torch.no_grad():\n",
        "            logps =  modelo(img.to(device)) #output do modelo en escala logaritmica\n",
        "\n",
        "        ps=torch.exp(logps) #converte output para escala normal(lembrando que é um tensor)\n",
        "        probab=list(ps.cpu().numpy()[0])\n",
        "        etiqueta_pred=probab.index(max(probab)) #converte o tensor em um número,no caso,o número que o modelo previu\n",
        "        etiqueta_certa = etiquetas.numpy()[i]\n",
        "        if(etiqueta_certa == etiqueta_pred): #compara a previsão com o valor correto\n",
        "          conta_corretas += 1\n",
        "        conta_todas += 1\n",
        "    print(\"Total de imagens testadas = \",conta_todas)\n",
        "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))\n",
        "                                                   "
      ],
      "metadata": {
        "id": "kO6gsL3yDozJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NX0niWXGC3B",
        "outputId": "ac5e52f9-7328-404d-af8c-dc597d551048"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (lineari): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}